{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biancasmrt/CAPACIT/blob/master/CS4650_p2_egoh9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxpid8J3_xt"
      },
      "source": [
        "# HW3 Programming Assignment\n",
        "\n",
        "In this assignment, we will train LSTM POS-taggers, and evaluate their performance.\n",
        "\n",
        "We will use English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
        "\n",
        "## Part 1 Building a Basic POS Tagger  [15 points]\n",
        "\n",
        "Define BasicPOSTagger -- 5 points\n",
        "\n",
        "\n",
        "Train and evaluate BasicPOSTagger -- 5 points\n",
        "\n",
        "\n",
        "Error analysis for BasicPOSTagger -- 5 points\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X367eCR3_x0"
      },
      "source": [
        "###Part 1.1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z77fkxDJGayB"
      },
      "source": [
        "**To begin this project, make a copy of this notebook and save it to your local drive so that you can edit it.**\n",
        "\n",
        "\n",
        "If you want GPU's (which will improve training speed), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "If you're new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2022/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch Basics notebook](http://bit.ly/pytorchbasics). Additionally, this [Text Sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch for NLP specific problems. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtnGNDoA3_x3"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY #\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# this is how we select a GPU if it's avalible on your computer or in the Colab environment.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEywStkc3M5"
      },
      "source": [
        "You can check to make sure a GPU is available using the following code block.\n",
        "\n",
        "If the below message is shown, it means you are using a CPU.\n",
        "```\n",
        "/bin/bash: nvidia-smi: command not found\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ounnp0ASc58O",
        "outputId": "fff80cd4-ad3a-4c0d-9b28-9e9dcf8a4bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 28 03:42:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    32W /  70W |   1016MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwA2y6OR3_yE"
      },
      "source": [
        "### Part 1.2 Preparing Data\n",
        "\n",
        "`train.txt`: The training data is present in this file. This file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hVu_ia9Ottg",
        "outputId": "c91d2df0-fff4-4fc3-f2ef-6faa7fdda09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2775k  100 2775k    0     0  12.3M      0 --:--:-- --:--:-- --:--:-- 12.3M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/train.txt > train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFpH2P1A3_yG",
        "outputId": "7a4680f8-68e5-46c3-ec28-c281086bf9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data:  7148\n",
            "Val Data:  1788\n",
            "Total tags:  44\n"
          ]
        }
      ],
      "source": [
        "def load_tag_data(tag_file):\n",
        "    all_sentences = []\n",
        "    all_tags = []\n",
        "    sent = []\n",
        "    tags = []\n",
        "    with open(tag_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                all_sentences.append(sent)\n",
        "                all_tags.append(tags)\n",
        "                sent = []\n",
        "                tags = []\n",
        "            else:\n",
        "                word, tag, _ = line.strip().split()\n",
        "                sent.append(word)\n",
        "                tags.append(tag)\n",
        "    return all_sentences, all_tags\n",
        "\n",
        "train_sentences, train_tags = load_tag_data('train.txt')\n",
        "\n",
        "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
        "\n",
        "# Create train-val split from train data\n",
        "train_val_data = list(zip(train_sentences, train_tags))\n",
        "random.shuffle(train_val_data)\n",
        "split = int(0.8 * len(train_val_data))\n",
        "training_data = train_val_data[:split]\n",
        "val_data = train_val_data[split:]\n",
        "\n",
        "print(\"Train Data: \", len(training_data))\n",
        "print(\"Val Data: \", len(val_data))\n",
        "print(\"Total tags: \", len(unique_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlfliN0J-RzV"
      },
      "source": [
        "### Part 1.3 Word-to-Index and Tag-to-Index mapping\n",
        "In order to work with text in Tensor format, we need to map each word to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uojEDun83_yP",
        "outputId": "050e536e-64cc-43b5-fcdb-652b4ddff95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tags 44\n",
            "Vocab size 19122\n"
          ]
        }
      ],
      "source": [
        "word_to_idx = {}\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "            \n",
        "tag_to_idx = {}\n",
        "for tag in unique_tags:\n",
        "    if tag not in tag_to_idx:\n",
        "        tag_to_idx[tag] = len(tag_to_idx)\n",
        "\n",
        "idx_to_tag = {}\n",
        "for tag in tag_to_idx:\n",
        "    idx_to_tag[tag_to_idx[tag]] = tag\n",
        "\n",
        "print(\"Total tags\", len(tag_to_idx))\n",
        "print(\"Vocab size\", len(word_to_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H26dqorp3_yX"
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(sent, idx_mapping):\n",
        "    idxs = [idx_mapping[word] for word in sent]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnBTCwD3_yc"
      },
      "source": [
        "### Part 1.4 Set up model\n",
        "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence.\n",
        "\n",
        "\n",
        "First we need to define some default hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P5SHabu3_yf"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkkS4oEb3_yk"
      },
      "source": [
        "### Part 1.5 Define Model\n",
        "\n",
        "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. These word embeddings act as input to the LSTM which produces a representation for each word. Then the representations of words are passed to a Linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCa30HQb3_ym"
      },
      "outputs": [],
      "source": [
        "class BasicPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(BasicPOSTagger, self).__init__()\n",
        "        #############################################################################\n",
        "        # TODO: Define and initialize anything needed for the forward pass.\n",
        "        # You are required to create a model with:\n",
        "        # an embedding layer: that maps words to the embedding space\n",
        "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
        "        # a linear layer: maps from hidden state space to tag space\n",
        "        #############################################################################\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
        "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, num_layers = LSTM_LAYERS)\n",
        "\n",
        "        # a linear layer: maps from hidden state space to tag space\n",
        "        self.linear_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # tag_scores = None\n",
        "        #############################################################################\n",
        "        # TODO: Implement the forward pass.\n",
        "        # Given a tokenized index-mapped sentence as the argument, \n",
        "        # compute the corresponding raw scores for tags (without softmax)\n",
        "        # returns:: tag_scores (Tensor)\n",
        "        #############################################################################\n",
        "        \n",
        "        x = self.word_embeddings(sentence)\n",
        "        x, _ = self.lstm_layer(x)\n",
        "        tag_scores = self.linear_layer(x)\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot9J3MrB3_ys"
      },
      "source": [
        "### Part 1.6 Training\n",
        "\n",
        "We define train and evaluate procedures that allow us to train our model using our created train-val split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWMGxh4Z3_yv"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        #############################################################################\n",
        "        # TODO: Implement the training method\n",
        "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
        "        # for sentences. Find the gradient with respect to the loss and update the\n",
        "        # model parameters using the optimizer.\n",
        "        #############################################################################\n",
        "        \n",
        "        #zero out the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #prepare input data (sentences and gold labels)\n",
        "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "        targets = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "        #do forward pass with current batch of input\n",
        "        pred_prob = model.forward(sentence_in.to(device))\n",
        "\n",
        "        #get loss with model predictions and true labels\n",
        "        loss = loss_function(pred_prob, targets.to(device))\n",
        "        loss.backward()\n",
        "\n",
        "        #update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        #increase running total loss and the number of past training samples \n",
        "        train_loss += loss.item()\n",
        "        train_examples += len(targets)\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "    \n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate(model, loss_function)\n",
        "        \n",
        "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
        "                                                                      EPOCHS, \n",
        "                                                                      avg_train_loss, \n",
        "                                                                      avg_val_loss,\n",
        "                                                                      val_accuracy))\n",
        "\n",
        "def evaluate(model, loss_function):\n",
        "  # returns:: avg_val_loss (float)\n",
        "  # returns:: val_accuracy (float)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            #############################################################################\n",
        "            # TODO: Implement the evaluate method\n",
        "            # Find the average validation loss along with the validation accuracy.\n",
        "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
        "            #############################################################################\n",
        "            \n",
        "            #prepare input data (sentences and gold labels)\n",
        "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "            targets = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            #do forward pass with current batch of input\n",
        "            pred_prob = model.forward(sentence_in.to(device))\n",
        "\n",
        "            #get loss with model predictions and true labels\n",
        "            loss = loss_function(pred_prob, targets.to(device))\n",
        "\n",
        "            #get the predicted labels\n",
        "            pred_labels = torch.argmax(pred_prob, dim=1)\n",
        "\n",
        "            #get number of correct prediction\n",
        "            for i in range(len(targets)):\n",
        "              if targets[i] == pred_labels[i]:\n",
        "                correct += 1\n",
        "\n",
        "            #increase running total loss and the number of past valid samples\n",
        "            val_loss += loss.item()\n",
        "            val_examples += len(targets)\n",
        "\n",
        "\n",
        "            #############################################################################\n",
        "            #                             END OF YOUR CODE                              #\n",
        "            #############################################################################\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsuHjjH1rQeS",
        "outputId": "bc933ed8-3629-494a-d4d9-9180c30a10d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 0.0630\tAvg Val Loss: 0.0492\t Val Accuracy: 68\n",
            "Epoch: 2/10\tAvg Train Loss: 0.0432\tAvg Val Loss: 0.0402\t Val Accuracy: 75\n",
            "Epoch: 3/10\tAvg Train Loss: 0.0353\tAvg Val Loss: 0.0352\t Val Accuracy: 79\n",
            "Epoch: 4/10\tAvg Train Loss: 0.0303\tAvg Val Loss: 0.0321\t Val Accuracy: 82\n",
            "Epoch: 5/10\tAvg Train Loss: 0.0268\tAvg Val Loss: 0.0298\t Val Accuracy: 83\n",
            "Epoch: 6/10\tAvg Train Loss: 0.0242\tAvg Val Loss: 0.0282\t Val Accuracy: 84\n",
            "Epoch: 7/10\tAvg Train Loss: 0.0222\tAvg Val Loss: 0.0270\t Val Accuracy: 85\n",
            "Epoch: 8/10\tAvg Train Loss: 0.0206\tAvg Val Loss: 0.0261\t Val Accuracy: 86\n",
            "Epoch: 9/10\tAvg Train Loss: 0.0193\tAvg Val Loss: 0.0254\t Val Accuracy: 86\n",
            "Epoch: 10/10\tAvg Train Loss: 0.0182\tAvg Val Loss: 0.0249\t Val Accuracy: 87\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10\n",
        "'''\n",
        "#############################################################################\n",
        "# TODO: Initialize the model, optimizer and the loss function\n",
        "#############################################################################\n",
        "from torch.optim import Adam\n",
        "\n",
        "model = BasicPOSTagger(embedding_dim = EMBEDDING_DIM,\n",
        "                       hidden_dim    = HIDDEN_DIM,\n",
        "                       vocab_size    = len(word_to_idx.keys()),\n",
        "                       tagset_size   = len(tag_to_idx.keys())).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "for epoch in range(1, EPOCHS + 1): \n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK6mT_k8NRvB"
      },
      "source": [
        "**Sanity Check!** Under the default hyperparameter setting, after 5 epochs you should be able to get at least 75% accuracy on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP64WDReBuDr"
      },
      "source": [
        "### Part 1.7 Error analysis\n",
        "\n",
        "In this step, we will analyze what kind of errors it was making on the validation set.\n",
        "\n",
        "Step 1, write a method to generate predictions from the validation set. For every sentence, get its words, predicted tags (model_tags), and the ground truth tags (gt_tags). To make the next step easier, you may want to concatenate words from all sentences into a very long list, and same for model_tags and gt_tags.\n",
        "\n",
        "\n",
        "Step 2, analyze what kind of errors the model was making. For example, it may frequently label NN as VB. Let's get the top-10 most frequent types of errors, each of their frequency, and some example words. One example is at below. It is interpreted as the model predicts NNP as VBG for 626 times, five random example words are shown.\n",
        "\n",
        "```\n",
        "['VBG', 'NNP', 626, ['Rowe', 'Livermore', 'Parker', 'F-16', 'HEYNOW']]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QgMHr7HCn1x",
        "outputId": "0b8575fc-3da9-4868-8882-4789cb7894b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NN', 'NNP'), 537, ['Ivy', 'Egon', 'Pennsylvania', 'Wenz', 'Barron']]\n",
            "[('JJ', 'NN'), 221, ['commercial', 'mother', 'hotdog', 'Source', 'net']]\n",
            "[('NNS', 'JJ'), 182, ['much-beloved', 'diverse', 'unable', 'sure', 'snooty']]\n",
            "[('NN', 'JJ'), 173, ['snake-oil', 'huge', 'bargain-basement', 'desperate', 'deflationary']]\n",
            "[('NNP', 'NN'), 146, ['novelist', 'Market', 'magazine', 'revolution', 'treasury']]\n",
            "[('VBD', 'VBN'), 123, ['ended', 'requested', 'continued', 'acquired', 'held']]\n",
            "[('NN', 'NNS'), 112, ['pleas', 'loopholes', 'restrictions', 'bills', 'Plans']]\n",
            "[('NNS', 'NNP'), 110, ['Galles', 'Allenport', 'Himont', 'Mitsuoka', 'Malapai']]\n",
            "[('NNS', 'CD'), 102, ['334,000', '617', '9.9', '142.43', '220']]\n",
            "[('NNS', 'NN'), 97, ['club', 'reflection', 'rolling', 're-election', 'chicken']]\n"
          ]
        }
      ],
      "source": [
        "#############################################################################\n",
        "# TODO: Generate predictions for val_data\n",
        "# Create lists of words, tags predicted by the model and ground truth tags.\n",
        "# Hint: It should look very similar to the evaluate function.\n",
        "#############################################################################\n",
        "def generate_predictions(model, val_data):\n",
        "    # returns:: word_list (str list)\n",
        "    # returns:: model_tags (str list) predicted tags\n",
        "    # returns:: gt_tags (str list)\n",
        "    # Your code here\n",
        "    word_list = []\n",
        "    model_tags = []\n",
        "    gt_tags = []\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \n",
        "            #prepare input data (sentences and gold labels)\n",
        "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "            targets = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            #do forward pass with current batch of input\n",
        "            tag_scores = model.forward(sentence_in.to(device))\n",
        "\n",
        "            #get the predicted labels\n",
        "            pred_labels = torch.argmax(tag_scores, 1)\n",
        "\n",
        "            #get number of correct prediction\n",
        "            for i in range(len(targets)):\n",
        "              model_tags.append(idx_to_tag[pred_labels.tolist()[i]])\n",
        "              gt_tags.append(tags[i])\n",
        "              word_list.append(sentence[i])\n",
        "    \n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Carry out error analysis\n",
        "# From those lists collected from the above method, find the \n",
        "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "# sorted by frequency\n",
        "# ['VBG', 'NNP', 626, ['Rowe', 'Livermore', 'Parker', 'F-16', 'HEYNOW']]\n",
        "# For example, it may frequently label NN as VB. \n",
        "# Let's get the top-10 most frequent types of errors, each of their frequency, and some example words.\n",
        "#############################################################################\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    # returns: errors (list of tuples)\n",
        "    # Your code here\n",
        "\n",
        "    error_dict = {}\n",
        "    example_words = {}\n",
        "    for i in range(len(model_tags)):\n",
        "      if (model_tags[i] != gt_tags[i]): \n",
        "        if (model_tags[i], gt_tags[i]) not in error_dict.keys():\n",
        "          error_dict[(model_tags[i], gt_tags[i])] = 1\n",
        "          example_words[(model_tags[i], gt_tags[i])] = [word_list[i]]\n",
        "        else:\n",
        "          error_dict[(model_tags[i], gt_tags[i])] += 1\n",
        "          example_words[(model_tags[i], gt_tags[i])].append(word_list[i])\n",
        "\n",
        "    sorted_error_dict = sorted(error_dict.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)\n",
        "    errors = []\n",
        "    for (tags, frequency) in sorted_error_dict:\n",
        "      errors.append([tags, frequency, example_words[tags][:5]])\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    \n",
        "    # '''\n",
        "    # NNP: Noun proper\n",
        "    # NN: Noun common\n",
        "    # JJ: Adjective\n",
        "    # NNS: Noun plural\n",
        "    # VBZ: verb 3rrd person singualr\n",
        "    # VBD: Verb singular\n",
        "    # VBN: verb past\n",
        "    # '''\n",
        "    #############################################################################\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNjFRDcD2h7"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors did the model make and why do you think it made them?\n",
        "\n",
        "The most frequent errors were the model over predicting common nouns and proper nouns. It mixed up the different types of nouns (common, singular, plural etc) which could be because sometimes common nouns are parts of titles which are proper nouns. It also predicted adjectives as nouns, maybe because they are uncommon and weren't seen in the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcI9BUZIo04"
      },
      "source": [
        "## Part 2 Hyper-parameter Tuning [10 points]\n",
        "\n",
        "In order to improve your model performance, try making some modifications on `EMBEDDING_DIM`, `HIDDEN_DIM`, and `LEARNING_RATE`. You will receive 50%/75%/100% credit for this section if your model, after being trained for 10 epochs, is able to achieve 80%/85%/90% accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RekmpLxzIo04",
        "outputId": "beec22f9-7deb-42fb-a0c7-74e20c135bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 0.0743\tAvg Val Loss: 0.0505\t Val Accuracy: 66\n",
            "Epoch: 2/10\tAvg Train Loss: 0.0416\tAvg Val Loss: 0.0353\t Val Accuracy: 77\n",
            "Epoch: 3/10\tAvg Train Loss: 0.0297\tAvg Val Loss: 0.0275\t Val Accuracy: 83\n",
            "Epoch: 4/10\tAvg Train Loss: 0.0230\tAvg Val Loss: 0.0231\t Val Accuracy: 85\n",
            "Epoch: 5/10\tAvg Train Loss: 0.0188\tAvg Val Loss: 0.0203\t Val Accuracy: 87\n",
            "Epoch: 6/10\tAvg Train Loss: 0.0159\tAvg Val Loss: 0.0185\t Val Accuracy: 89\n",
            "Epoch: 7/10\tAvg Train Loss: 0.0137\tAvg Val Loss: 0.0172\t Val Accuracy: 89\n",
            "Epoch: 8/10\tAvg Train Loss: 0.0120\tAvg Val Loss: 0.0162\t Val Accuracy: 90\n",
            "Epoch: 9/10\tAvg Train Loss: 0.0105\tAvg Val Loss: 0.0155\t Val Accuracy: 91\n",
            "Epoch: 10/10\tAvg Train Loss: 0.0093\tAvg Val Loss: 0.0149\t Val Accuracy: 91\n"
          ]
        }
      ],
      "source": [
        "YOUR_EMBEDDING_DIM = 10\n",
        "YOUR_HIDDEN_DIM = 20\n",
        "YOUR_LEARNING_RATE = 0.001\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Set three hyper-parameters. Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "#############################################################################\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "model_tuned = BasicPOSTagger(embedding_dim = YOUR_EMBEDDING_DIM,\n",
        "                       hidden_dim    = YOUR_HIDDEN_DIM,\n",
        "                       vocab_size    = len(word_to_idx.keys()),\n",
        "                       tagset_size   = len(tag_to_idx.keys())).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_tuned = torch.optim.Adam(model_tuned.parameters(), lr = YOUR_LEARNING_RATE)\n",
        "\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "for epoch in range(1, EPOCHS + 1): \n",
        "    train(epoch, model_tuned, loss_function, optimizer_tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXyUssdXZ4r"
      },
      "source": [
        "## Part 3 Character-level POS Tagger  [15 points]\n",
        "\n",
        "Define CharPOSTagger -- 5 points\n",
        "\n",
        "Train and evaluate CharPOSTagger -- 5 points\n",
        "\n",
        "Error analysis for CharPOSTagger -- 5 points\n",
        "\n",
        "Use the character-level information to augment word embeddings. For example, words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character-level LSTM on every word to create a character-level representation of the word. Take the last hidden state from the character-level LSTM as the representation and concatenate with the word embedding (as in the BasicPOSTagger) to create a new word representation that captures more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX4-3AoxSJeY"
      },
      "outputs": [],
      "source": [
        "# Create char to index mapping\n",
        "char_to_idx = {}\n",
        "unique_chars = set()\n",
        "MAX_WORD_LEN = 0\n",
        "\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        for c in word:\n",
        "            unique_chars.add(c)\n",
        "        if len(word) > MAX_WORD_LEN:\n",
        "            MAX_WORD_LEN = len(word)\n",
        "\n",
        "for c in unique_chars:\n",
        "    char_to_idx[c] = len(char_to_idx)\n",
        "char_to_idx[' '] = len(char_to_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xXPsL3nDjAu"
      },
      "source": [
        "### How to do padding correctly for the characters?\n",
        "\n",
        "\n",
        "Assume we have got a sentence [\"We\", \"love\", \"NLP\"]. You are supposed to first prepend a certain number of blank characters to each of the words in this sentence.\n",
        "\n",
        "How to determine the number of blank characters we need? The calculation of MAX_WORD_LEN is here for help (which we already provide in the starter code). For the given sentence, MAX_WORD_LEN equals 4. Therefore we prepend two blank characters to \"We\", zero blank character to \"love\", and one blank character to \"NLP\". So the resultant padded sentence we get should be [\"  We\", \"love\", \" NLP\"].\n",
        "\n",
        "Then, we feed all characters in [\"  We\", \"love\", \" NLP\"] into a char-embedding layer, and get a tensor of shape (3, 4, char_embedding_dim). To make this tensor's shape proper for the char-level LSTM (nn.LSTM), we need to transpose this tensor, i.e. swap the first and the second dimension. So we get a tensor of shape (4, 3, char_embedding_dim), where 4 corresponds to seq_len and 3 corresponds to batch_size.\n",
        "\n",
        "The last thing you need to do is to obtain the last hidden state from the char-level LSTM, and concatenate it with the word embedding, so that you can get an augmented representation of that word.\n",
        "\n",
        "[This](https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/char_padding.png) is an illustration for left padding characters.\n",
        "\n",
        "### Why doing the padding?\n",
        "Someone may ask why we want to do such a kind of padding, instead of directly passing each of the character sequences of each word one by one through an LSTM, to get the last hidden state. The reason is that if you don't do padding, then that means you can only implement this process using \"for loop\". For CharPOSTagger, if you implement it using \"for loop\", the training time would be approximately 150s (GPU) / 250s (CPU) per epoch, while it would be around 30s (GPU) / 150s (CPU) per epoch if you do the padding and feed your data in batches. Therefore, we strongly recommend you learn how to do the padding and transform your data into batches. In fact, those are quite important concepts which you should get yourself familar with, although it might take you some time.\n",
        "\n",
        "### Why doing left padding?\n",
        "Our hypothesis is that the suffixes of English words (e.g., -ly, -ing, etc) are more indicative than prefixes for the part-of-speech (POS). Though LSTM is supposed to be able to handle long sequences, it still lose information along the way and the information closer to the last state (which you use as char-level representations) will be retained better. \n",
        "\n",
        "### How to understand the dimention change?\n",
        "Assume we have got a sentence with 3 words [\"We\", \"love\", \"NLP\"], and assume the dimension of character embedding is 2, the dimension of word embedding is 4, the dimension of word-level LSTM's hidden layer is 5, the dimension of character-level LSTM's hidden layer is 6.\n",
        "\n",
        "In BasicPOSTagger, the dimension change would be (3x1x4) ----word-level LSTM----> (3x1x5) ----linear layer----> (3x1x44).\n",
        "\n",
        "In CharPOSTagger, after padding, character embedding, and swapping, the dimension change would be (MAX_WORD_LEN, 3, 2) ----character-level LSTM----> (MAX_WORD_LEN, 3, 6) ----Take the last hidden state----> (3, 6) ----concatenate with word embedings----> (3x1x10) ----word-level LSTM----> (3x1x5) ----linear layer----> (3x1x44)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMsoXAMDO9-6"
      },
      "outputs": [],
      "source": [
        "# New Hyperparameters\n",
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10\n",
        "CHAR_EMBEDDING_DIM = 4\n",
        "CHAR_HIDDEN_DIM = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U0wb4OeOsde"
      },
      "outputs": [],
      "source": [
        "class CharPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
        "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
        "        super(CharPOSTagger, self).__init__()\n",
        "        #############################################################################\n",
        "        # TODO: Define and initialize anything needed for the forward pass.\n",
        "        # You are required to create a model with:\n",
        "        # an embedding layer for word: that maps words to their embedding space\n",
        "        # an embedding layer for character: that maps characters to their embedding space\n",
        "        # a character-level LSTM layer: that finds the character-level embedding for a word\n",
        "        # a word-level LSTM layer: that takes the concatenated representation per word (word embedding + char-lstm) as input and outputs hidden states\n",
        "        # a linear layer: maps from hidden state space to tag space\n",
        "        #############################################################################\n",
        "\n",
        "        # an embedding layer for word: that maps words to their embedding space\n",
        "        self.word_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # an embedding layer for character: that maps characters to their embedding space\n",
        "        self.char_embedding_layer = nn.Embedding(char_size, char_embedding_dim)\n",
        "\n",
        "        # a character-level LSTM layer: that finds the character-level embedding for a word\n",
        "        self.char_lstm_layer = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
        "        \n",
        "        # a word-level LSTM layer: that takes the concatenated representation per word (word embedding + char-lstm) as input and outputs hidden states\n",
        "        self.lstm_layer = nn.LSTM(embedding_dim + char_hidden_dim, hidden_dim)\n",
        "        \n",
        "        # a linear layer: maps from hidden state space to tag space\n",
        "        self.linear_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "\n",
        "    def forward(self, sentence, chars):\n",
        "        # tag_scores = None\n",
        "        #############################################################################\n",
        "        # TODO: Implement the forward pass.\n",
        "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
        "        # find the corresponding raw scores for tags (without softmax)\n",
        "        # returns:: tag_scores (Tensor)\n",
        "\n",
        "        # Then, we feed all characters in [\" We\", \"love\", \" NLP\"] into a char-embedding layer,\n",
        "        # and get a tensor of shape (3, 4, char_embedding_dim). To make this tensor's shape proper for the char-level LSTM (nn.LSTM), \n",
        "        # we need to transpose this tensor, i.e. swap the first and the second dimension. \n",
        "        # So we get a tensor of shape (4, 3, char_embedding_dim), where 4 corresponds to seq_len and 3 corresponds to batch_size.\n",
        "        #############################################################################\n",
        "\n",
        "        word_embeds = self.word_embedding_layer(sentence)\n",
        "        char_embeds = self.char_embedding_layer(chars)\n",
        "        torch.transpose(char_embeds, 0, 1)\n",
        "        # print(\"char_embeds: \", char_embeds.shape)\n",
        "        char_lstm_out, _ = self.char_lstm_layer(char_embeds)\n",
        "        \n",
        "        # take the end of the sequence\n",
        "        end_char_embedding = char_lstm_out[:, -1, :]\n",
        "        # print(\"word_embeds: \", word_embeds.shape)\n",
        "        # print(\"end_char_embedding: \", end_char_embedding.shape)\n",
        "        combined = torch.cat((word_embeds, end_char_embedding), dim=1)\n",
        "        lstm_out, _ = self.lstm_layer(combined)\n",
        "        tag_scores = self.linear_layer(lstm_out)\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "        return tag_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll3IHzmiSxf6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_char(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        #############################################################################\n",
        "        # TODO: Implement the training method\n",
        "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
        "        # for sentences. For constructing character input, you may want to left pad\n",
        "        # each word to MAX_WORD_LEN first, then use prepare_sequence method to create\n",
        "        # index  mappings. \n",
        "        #############################################################################\n",
        "        \n",
        "        #zero out the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #prepare input data (sentences, characters, and gold labels)\n",
        "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "        targets = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "        encoded_words = list()\n",
        "        # print(\"sentence: \", len(sentence))\n",
        "\n",
        "        for word in sentence:\n",
        "          # word = word.rjust(MAX_WORD_LEN, ' ')\n",
        "          word = ' ' * (MAX_WORD_LEN - len(word)) + word\n",
        "          character_in = prepare_sequence(word, char_to_idx)\n",
        "          encoded_words.append(character_in.resize_(1, MAX_WORD_LEN))\n",
        "        encoded_words = torch.cat(encoded_words, 0)\n",
        "\n",
        "        #do forward pass with current batch of input\n",
        "        pred_prob = model.forward(sentence_in.to(device), encoded_words.to(device))\n",
        "\n",
        "        #get loss with model predictions and true labels\n",
        "        loss = loss_function(pred_prob, targets.to(device))\n",
        "        loss.backward()\n",
        "\n",
        "        #update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        #increase running total loss and the number of past training samples \n",
        "        train_loss += loss.item()\n",
        "        train_examples += len(targets)\n",
        "\n",
        "        #############################################################################\n",
        "        #                             END OF YOUR CODE                              #\n",
        "        #############################################################################\n",
        "    \n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function)\n",
        "        \n",
        "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
        "                                                                      EPOCHS, \n",
        "                                                                      avg_train_loss, \n",
        "                                                                      avg_val_loss,\n",
        "                                                                      val_accuracy))\n",
        "\n",
        "def evaluate_char(model, loss_function):\n",
        "    # returns:: avg_val_loss (float)\n",
        "    # returns:: val_accuracy (float)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            #############################################################################\n",
        "            # TODO: Implement the evaluate method\n",
        "            # Find the average validation loss along with the validation accuracy.\n",
        "            # Hint: To find the accuracy, argmax of tag predictions can be used. \n",
        "            #############################################################################\n",
        "\n",
        "            #prepare input data (sentences, characters, and gold labels)\n",
        "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "            targets = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            encoded_words = list()\n",
        "            for word in sentence:\n",
        "              word = ' ' * (MAX_WORD_LEN - len(word)) + word\n",
        "              character_in = prepare_sequence(word, char_to_idx)\n",
        "              encoded_words.append(character_in.resize_(1, MAX_WORD_LEN))\n",
        "            encoded_words = torch.cat(encoded_words, 0)\n",
        "\n",
        "            #do forward pass with current batch of input\n",
        "            pred_prob = model.forward(sentence_in.to(device), encoded_words.to(device))\n",
        "\n",
        "            #get loss with model predictions and true labels\n",
        "            loss = loss_function(pred_prob, targets.to(device))\n",
        "\n",
        "            #get the predicted labels\n",
        "            pred_labels = torch.argmax(pred_prob, dim=1)\n",
        "\n",
        "            #get number of correct prediction\n",
        "            for i in range(len(targets)):\n",
        "              if targets[i] == pred_labels[i]:\n",
        "                correct += 1\n",
        "            #increase running total loss and the number of past valid samples\n",
        "            val_loss += loss.item()\n",
        "            val_examples += len(targets)\n",
        "\n",
        "            #############################################################################\n",
        "            #                             END OF YOUR CODE                              #\n",
        "            #############################################################################\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-QttCw6Otf-",
        "outputId": "582dad57-a123-424f-da17-5d4348a9b2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 1.1944\tAvg Val Loss: 0.8803\t Val Accuracy: 74\n",
            "Epoch: 2/10\tAvg Train Loss: 0.7410\tAvg Val Loss: 0.6854\t Val Accuracy: 82\n",
            "Epoch: 3/10\tAvg Train Loss: 0.5827\tAvg Val Loss: 0.5883\t Val Accuracy: 85\n",
            "Epoch: 4/10\tAvg Train Loss: 0.4935\tAvg Val Loss: 0.5308\t Val Accuracy: 86\n",
            "Epoch: 5/10\tAvg Train Loss: 0.4324\tAvg Val Loss: 0.4905\t Val Accuracy: 88\n",
            "Epoch: 6/10\tAvg Train Loss: 0.3886\tAvg Val Loss: 0.4628\t Val Accuracy: 88\n",
            "Epoch: 7/10\tAvg Train Loss: 0.3506\tAvg Val Loss: 0.4364\t Val Accuracy: 89\n",
            "Epoch: 8/10\tAvg Train Loss: 0.3223\tAvg Val Loss: 0.4200\t Val Accuracy: 90\n",
            "Epoch: 9/10\tAvg Train Loss: 0.3005\tAvg Val Loss: 0.4081\t Val Accuracy: 90\n",
            "Epoch: 10/10\tAvg Train Loss: 0.2824\tAvg Val Loss: 0.3983\t Val Accuracy: 90\n"
          ]
        }
      ],
      "source": [
        "#############################################################################\n",
        "# TODO: Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "# New Hyperparameters\n",
        "# EMBEDDING_DIM = 4\n",
        "# HIDDEN_DIM = 8\n",
        "# LEARNING_RATE = 0.1\n",
        "# LSTM_LAYERS = 1\n",
        "# DROPOUT = 0\n",
        "# EPOCHS = 10\n",
        "# CHAR_EMBEDDING_DIM = 4\n",
        "# CHAR_HIDDEN_DIM = 4\n",
        "\n",
        "# self, embedding_dim, hidden_dim, char_embedding_dim, \n",
        "#                  char_hidden_dim, char_size, vocab_size, tagset_size\n",
        "\n",
        "#############################################################################\n",
        "model = CharPOSTagger(embedding_dim = EMBEDDING_DIM,\n",
        "                      hidden_dim    = HIDDEN_DIM,\n",
        "                      char_embedding_dim = CHAR_HIDDEN_DIM,\n",
        "                      char_hidden_dim = CHAR_HIDDEN_DIM,\n",
        "                      char_size = len(char_to_idx.keys()),\n",
        "                      vocab_size    = len(word_to_idx.keys()),\n",
        "                      tagset_size   = len(tag_to_idx.keys())).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "#                             END OF YOUR CODE                              #\n",
        "#############################################################################\n",
        "for epoch in range(1, EPOCHS + 1): \n",
        "    train_char(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xslNYW8EBKMQ"
      },
      "source": [
        "**Sanity Check!** Under the default hyperparameter setting, after 5 epochs you should be able to get at least 85% accuracy on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsHtaCQIo05"
      },
      "source": [
        "### Part 3.1 Error analysis\n",
        "Write a method to generate predictions for the validation set.\n",
        "Create lists of words, tags predicted by the model and ground truth tags. \n",
        "\n",
        "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model.\n",
        "\n",
        "This part is very similar to part 1.7. You may want to refer to your implementation there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vUawGsWIo06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc243860-d5ba-4a3f-ef65-be659082fff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NN', 'NNP'), 246, ['Harrison', 'Hickman', 'English', 'CSC', 'Himont']]\n",
            "[('JJ', 'NN'), 198, ['commercial', 'hotdog', 'corridor', 'stockpile', 'net']]\n",
            "[('NNP', 'NN'), 176, ['award', 'park', 'Market', 'softness', 'tandem']]\n",
            "[('JJ', 'NNP'), 158, ['Bryan', 'mature', 'British', 'Telerate', 'Employee']]\n",
            "[('NN', 'JJ'), 154, ['snake-oil', 'gullible', 'previous', '60-inch', 'net']]\n",
            "[('NNP', 'JJ'), 153, ['plain', 'California', 'West', 'FEDERAL', 'South']]\n",
            "[('VBN', 'VBD'), 149, ['held', 'permitted', 'ended', 'conceded', 'Warned']]\n",
            "[('NNS', 'VBZ'), 131, ['implies', 'plans', 'approaches', 'targets', 'follows']]\n",
            "[('VBD', 'VBN'), 123, ['made', 'ended', 'requested', 'acquired', 'offered']]\n",
            "[('JJ', 'VBP'), 108, ['have', 'see', 'have', 'have', 'get']]\n"
          ]
        }
      ],
      "source": [
        "#############################################################################\n",
        "# TODO: Generate predictions for val_data\n",
        "# Create lists of words, tags predicted by the model and ground truth tags.\n",
        "# Hint: It should look very similar to the evaluate function.\n",
        "#############################################################################\n",
        "def generate_predictions(model, val_data):\n",
        "    # returns:: word_list (str list)\n",
        "    # returns:: model_tags (str list)\n",
        "    # returns:: gt_tags (str list)\n",
        "    # Your code here\n",
        "\n",
        "    word_list = []\n",
        "    model_tags = []\n",
        "    gt_tags = []\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \n",
        "            #prepare input data (sentences and gold labels)\n",
        "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
        "            targets = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            encoded_words = list()\n",
        "            for word in sentence:\n",
        "              word = ' ' * (MAX_WORD_LEN - len(word)) + word\n",
        "              character_in = prepare_sequence(word, char_to_idx)\n",
        "              encoded_words.append(character_in.resize_(1, MAX_WORD_LEN))\n",
        "            encoded_words = torch.cat(encoded_words, 0)\n",
        "\n",
        "            #do forward pass with current batch of input\n",
        "            pred_prob = model.forward(sentence_in.to(device), encoded_words.to(device))\n",
        "\n",
        "            #get the predicted labels\n",
        "            pred_labels = torch.argmax(pred_prob, 1)\n",
        "\n",
        "            #get number of correct prediction\n",
        "            for i in range(len(targets)):\n",
        "              model_tags.append(idx_to_tag[pred_labels.tolist()[i]])\n",
        "              gt_tags.append(tags[i])\n",
        "              word_list.append(sentence[i])\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Carry out error analysis\n",
        "# From those lists collected from the above method, find the \n",
        "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "# sorted by frequency\n",
        "#############################################################################\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    # returns: errors (list of tuples)\n",
        "    # Your code here\n",
        "\n",
        "    error_dict = {}\n",
        "    example_words = {}\n",
        "    for i in range(len(model_tags)):\n",
        "      if (model_tags[i] != gt_tags[i]):\n",
        "        if (model_tags[i], gt_tags[i]) not in error_dict.keys():\n",
        "          error_dict[(model_tags[i], gt_tags[i])] = 1\n",
        "          example_words[(model_tags[i], gt_tags[i])] = [word_list[i]]\n",
        "        else:\n",
        "          error_dict[(model_tags[i], gt_tags[i])] += 1\n",
        "          example_words[(model_tags[i], gt_tags[i])].append(word_list[i])\n",
        "\n",
        "    sorted_error_dict = sorted(error_dict.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)\n",
        "    errors = []\n",
        "    for (tags, frequency) in sorted_error_dict:\n",
        "      errors.append([tags, frequency, example_words[tags][:5]])\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLl_BSMeovb"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? \n",
        "\n",
        "In the original model, a lot of the mistakes the model made were that it predicted things to be nouns incorrectly. In this model, the model is still incorrectly predicting words to be nouns incorrectly, but less than in the original model. This is probably because certain suffixes can lead the model to predict a words to be an adverb (-ly) or present tense verb (-ing) etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W09rDJA03pcT"
      },
      "source": [
        "### Part 4: Submit Your Homework\n",
        "This is the end. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/345683):\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p2_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended. \n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 1.6 & 2 & 3 are captured so we can see how the loss and accuracy changes while training.\n",
        "5. Upload all 3 files to GradeScope.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}